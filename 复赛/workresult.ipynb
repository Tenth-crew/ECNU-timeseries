{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    统计指定目录下的文件数量。\n",
    "\n",
    "    :param directory_path: 要检查的目录路径\n",
    "    :return: 文件数量\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 获取目录下的所有文件和子目录\n",
    "        entries = os.listdir(directory_path)\n",
    "        \n",
    "        # 统计文件数量\n",
    "        file_count = sum(1 for entry in entries if os.path.isfile(os.path.join(directory_path, entry)))\n",
    "        \n",
    "        return file_count\n",
    "    except FileNotFoundError:\n",
    "        print(f\"目录 {directory_path} 不存在\")\n",
    "        return 0\n",
    "    except PermissionError:\n",
    "        print(f\"没有权限访问目录 {directory_path}\")\n",
    "        return 0\n",
    "\n",
    "# 示例用法\n",
    "directory_path = 'dataset/Top500'\n",
    "file_count = count_files_in_directory(directory_path)\n",
    "print(f\"目录 {directory_path} 下的文件数量: {file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Found metrics without a dataset name.\n",
      "Summary for 84_84 saved to workresult/10dataset_roll_ewma/test_Result_span28_84_84.csv\n"
     ]
    }
   ],
   "source": [
    "# test 获取数据\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# 日志文件路径列表\n",
    "# log_files = [\n",
    "#     'logs/test_patchtst_28_28_28.log',\n",
    "#     'logs/test_patchtst_56_28_28.log',\n",
    "#     'logs/test_patchtst_56_56_56.log',\n",
    "#     'logs/test_patchtst_84_56_56.log',\n",
    "#     'logs/test_patchtst_84_84_84.log'\n",
    "# ]\n",
    "\n",
    "log_files = [\n",
    "    'logs/test_patchtst_84_84.log'\n",
    "]\n",
    "\n",
    "# 正则表达式模式，用于匹配数据集名称 这里捕获的数据结果记得和和下面的路径匹配\n",
    "# dataset_pattern = re.compile(r'Data Path:\\s*(.+?)_all_ewma_span20.csv.*')\n",
    "# dataset_pattern = re.compile(r'Data Path:\\s*(.+?)_all.csv.*')\n",
    "# dataset_pattern = re.compile(r'Data Path:\\s*(.+?)_all_roll_ewma_span20.csv.*')\n",
    "# dataset_pattern = re.compile(r'Data Path:\\s*(.+?)_all_ewma_span28.csv.*')\n",
    "# dataset_pattern = re.compile(r'Data Path:\\s*(.+?)_all_roll.csv.*')\n",
    "dataset_pattern = re.compile(r'Data Path:\\s*(.+?)_all_roll_ewma_span28.csv.*')\n",
    "\n",
    "\n",
    "# 正则表达式模式，用于匹配 MSE 和 MAE\n",
    "metrics_pattern = re.compile(r'mse:\\s*(nan|[0-9.e+-]+),\\s*mae:\\s*(nan|[0-9.e+-]+),\\s*dtw:-999')\n",
    "\n",
    "# 正则表达式模式，用于匹配 seq_len, label_len, pred_len\n",
    "# config_pattern = re.compile(r'test_patchtst_(\\d+)_(\\d+)_(\\d+)\\.log')\n",
    "# 下面这个用来匹配 84 84\n",
    "config_pattern = re.compile(r'test_patchtst_(\\d+)_(\\d+)\\.log')\n",
    "\n",
    "# 逐个处理日志文件\n",
    "for log_file_path in log_files:\n",
    "    # 提取 seq_len, label_len, pred_len\n",
    "    config_match = config_pattern.search(log_file_path)\n",
    "    if config_match:\n",
    "        # seq_len, label_len, pred_len = config_match.groups()\n",
    "        label_len, pred_len = config_match.groups()\n",
    "        # 输出文件路径记得修改 ！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "        # csv_output_path = f'workresult/10dataset_roll_ewma/test_Result_adjustlr_{seq_len}_{label_len}_{pred_len}.csv'\n",
    "        csv_output_path = f'workresult/10dataset_roll_ewma/test_Result_span28.csv'\n",
    "    else:\n",
    "        print(f\"Warning: Could not extract configuration from {log_file_path}.\")\n",
    "        continue\n",
    "    \n",
    "    # 初始化数据集名称\n",
    "    dataset_name = None\n",
    "\n",
    "    # 打开 CSV 文件\n",
    "    with open(csv_output_path, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        # 写入 CSV 文件的表头\n",
    "        writer.writerow(['dataset', 'seq_len', 'label_len', 'pred_len', 'mse', 'mae'])\n",
    "        \n",
    "        # 打开日志文件\n",
    "        with open(log_file_path, 'r') as log_file:\n",
    "            # 逐行读取日志文件\n",
    "            for line in log_file:\n",
    "                # 匹配数据集名称\n",
    "                dataset_match = dataset_pattern.search(line)\n",
    "                if dataset_match:\n",
    "                    dataset_name = dataset_match.group(1)\n",
    "                \n",
    "                # 匹配 MSE 和 MAE\n",
    "                metrics_match = metrics_pattern.search(line)\n",
    "                if metrics_match:\n",
    "                    mse, mae = metrics_match.groups()\n",
    "                    # 将 MSE 和 MAE 保留到小数点后三位\n",
    "                    mse = f\"{float(mse):.3f}\"\n",
    "                    mae = f\"{float(mae):.3f}\"\n",
    "                    # 写入 CSV 文件\n",
    "                    if dataset_name:\n",
    "                        # writer.writerow([dataset_name, seq_len, label_len, pred_len, mse, mae])\n",
    "                        writer.writerow([dataset_name, label_len, pred_len, mse, mae])\n",
    "                        dataset_name = None\n",
    "                    else:\n",
    "                        print(\"Warning: Found metrics without a dataset name.\")\n",
    "\n",
    "    # print(f\"Summary for {seq_len}_{label_len}_{pred_len} saved to {csv_output_path}\")\n",
    "    print(f\"Summary for {label_len}_{pred_len} saved to {csv_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Found metrics without a dataset name.\n",
      "Summary\n",
      "Warning: Found metrics without a dataset name.\n",
      "Summary\n",
      "Warning: Found metrics without a dataset name.\n",
      "Summary\n",
      "Warning: Found metrics without a dataset name.\n",
      "Summary\n",
      "Warning: Found metrics without a dataset name.\n",
      "Summary\n",
      "Warning: Found metrics without a dataset name.\n",
      "Summary\n",
      "Warning: Found metrics without a dataset name.\n",
      "Summary\n",
      "Warning: Found metrics without a dataset name.\n",
      "Summary\n",
      "Warning: Found metrics without a dataset name.\n",
      "Summary\n",
      "Warning: Found metrics without a dataset name.\n",
      "Summary\n"
     ]
    }
   ],
   "source": [
    "# 从多个日志文件中提取出mse信息\n",
    "import csv\n",
    "import re\n",
    "\n",
    "repoName = ['vue', 'tensorflow', 'autogpt', 'kubernetes', 'terminal',\n",
    "            'flutter', 'vscode', 'react-naive', 'electron', 'transformers']\n",
    "\n",
    "# dataset_pattern = re.compile(r'Data Path:\\s*(.+?)_all_roll_ewma_span28_normalize_Quartiles.csv*')\n",
    "dataset_pattern = re.compile(r'Data Path:\\s*(.+?)_all_roll_ewma_span28_normalize.csv*')\n",
    "\n",
    "# 正则表达式模式，用于匹配 MSE 和 MAE\n",
    "metrics_pattern = re.compile(r'mse:\\s*(nan|[0-9.e+-]+),\\s*mae:\\s*(nan|[0-9.e+-]+),\\s*dtw:-999')\n",
    "\n",
    "for name in repoName:\n",
    "    log_files = f'logs/{name}_patchtst_84_84.log'\n",
    "\n",
    "    # csv_output_path = f'workresult/10dataset_roll_ewma_normalize/Result_span28_normalize_quartiles.csv'\n",
    "    csv_output_path = f'workresult/10dataset_roll_ewma_normalize/Result_span28_normalize.csv'\n",
    "\n",
    "    # 初始化数据集名称\n",
    "    dataset_name = None\n",
    "\n",
    "    # 检查文件是否为空\n",
    "    file_is_empty = False\n",
    "    try:\n",
    "        with open(csv_output_path, 'r', newline='') as csv_file:\n",
    "            file_is_empty = csv_file.read().strip() == ''\n",
    "    except FileNotFoundError:\n",
    "        file_is_empty = True\n",
    "\n",
    "    # 打开 CSV 文件\n",
    "    with open(csv_output_path, 'a', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        # 如果文件为空，则写入表头\n",
    "        if file_is_empty:\n",
    "            writer.writerow(['dataset', 'mse', 'mae'])\n",
    "        \n",
    "        # 打开日志文件\n",
    "        with open(log_files, 'r') as log_file:\n",
    "            # 逐行读取日志文件\n",
    "            for line in log_file:\n",
    "                # 匹配数据集名称\n",
    "                dataset_match = dataset_pattern.search(line)\n",
    "                if dataset_match:\n",
    "                    dataset_name = dataset_match.group(1)\n",
    "                \n",
    "                # 匹配 MSE 和 MAE\n",
    "                metrics_match = metrics_pattern.search(line)\n",
    "                if metrics_match:\n",
    "                    mse, mae = metrics_match.groups()\n",
    "                    # 将 MSE 和 MAE 保留到小数点后三位\n",
    "                    mse = f\"{float(mse):.3f}\"\n",
    "                    mae = f\"{float(mae):.3f}\"\n",
    "                    # 写入 CSV 文件\n",
    "                    if dataset_name:\n",
    "                        writer.writerow([dataset_name, mse, mae])\n",
    "                        dataset_name = None\n",
    "                    else:\n",
    "                        print(\"Warning: Found metrics without a dataset name.\")\n",
    "\n",
    "    # print(f\"Summary for {seq_len}_{label_len}_{pred_len} saved to {csv_output_path}\")\n",
    "    print(\"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary done\n"
     ]
    }
   ],
   "source": [
    "# 文件夹结果统计\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 正则表达式模式，用于匹配 MSE 和 MAE\n",
    "metrics_pattern = re.compile(r'mse:\\s*(nan|[0-9.e+-]+),\\s*mae:\\s*(nan|[0-9.e+-]+),\\s*dtw:-999')\n",
    "\n",
    "dataset_pattern = re.compile(r'Data Path:\\s*(.+?)_all_roll_ewma_span28_normalize.csv*')\n",
    "\n",
    "# 获取 logs 文件夹下以 top 开头的文件\n",
    "# log_files = [f for f in os.listdir('logs') if f.startswith('Top')]\n",
    "# log_files = [f for f in os.listdir('logs') if f.startswith('Bottom')]\n",
    "log_files = [f for f in os.listdir('logs') if f.startswith('language')]\n",
    "\n",
    "# csv 输出路径\n",
    "# csv_output_path = 'workresult/Top100_random_roll_ewma_normalize/Result.csv'\n",
    "# csv_output_path = 'workresult/Bottom100_random_roll_ewma_normalize/Result.csv'\n",
    "csv_output_path = 'workresult/language/Result.csv'\n",
    "\n",
    "# 初始化数据集名称\n",
    "dataset_name = None\n",
    "\n",
    "# 检查文件是否为空\n",
    "file_is_empty = False\n",
    "try:\n",
    "    with open(csv_output_path, 'r', newline='') as csv_file:\n",
    "        file_is_empty = csv_file.read().strip() == ''\n",
    "except FileNotFoundError:\n",
    "    file_is_empty = True\n",
    "\n",
    "# 打开 CSV 文件\n",
    "with open(csv_output_path, 'a', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    # 如果文件为空，则写入表头\n",
    "    if file_is_empty:\n",
    "        writer.writerow(['dataset', 'mse', 'mae'])\n",
    "\n",
    "    # 处理每个日志文件\n",
    "    for log_file_name in log_files:\n",
    "        log_file_path = os.path.join('logs', log_file_name)\n",
    "\n",
    "        # 初始化数据集名称\n",
    "        dataset_name = None\n",
    "\n",
    "        # 打开日志文件\n",
    "        with open(log_file_path, 'r') as log_file:\n",
    "            # 逐行读取日志文件\n",
    "            for line in log_file:\n",
    "                # 匹配数据集名称\n",
    "                dataset_match = dataset_pattern.search(line)\n",
    "                if dataset_match:\n",
    "                    dataset_name = dataset_match.group(1)\n",
    "                \n",
    "                # 匹配 MSE 和 MAE\n",
    "                metrics_match = metrics_pattern.search(line)\n",
    "                if metrics_match:\n",
    "                    mse, mae = metrics_match.groups()\n",
    "                    # 将 MSE 和 MAE 保留到小数点后三位\n",
    "                    mse = f\"{float(mse):.3f}\" if mse != 'nan' else mse\n",
    "                    mae = f\"{float(mae):.3f}\" if mae != 'nan' else mae\n",
    "                    # 写入 CSV 文件\n",
    "                    if dataset_name:\n",
    "                        writer.writerow([dataset_name, mse, mae])\n",
    "                        dataset_name = None\n",
    "                    else:\n",
    "                        print(\"Warning: Found metrics without a dataset name.\")\n",
    "\n",
    "print(\"Summary done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取原始CSV文件\n",
    "file_path = 'workresult/language/Result.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 找出包含nan值的行\n",
    "nan_rows = df[df.isnull().any(axis=1)]\n",
    "\n",
    "# 提取被删除的行的dataset属性\n",
    "deleted_datasets = nan_rows['dataset']\n",
    "\n",
    "# 保存被删除的行的dataset属性到另一个CSV文件\n",
    "deleted_datasets.to_csv('workresult/language/Deleted_Datasets.csv', index=False, header=['dataset'])\n",
    "\n",
    "# 删除包含nan值的行\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# 保存处理后的CSV文件\n",
    "df_cleaned.to_csv('workresult/language/Cleaned_Result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 test_Result_span28_84_84_84_localZscore.csv 的 mse 均值: 1.6369\n",
      "文件 test_Result_span28_84_84.csv 的 mse 均值: 0.4318000000000001\n",
      "所有文件的 mse 均值的均值: 1.03435\n"
     ]
    }
   ],
   "source": [
    "# 验证哪个参数配置效果好\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_mse_mean(directory):\n",
    "    # 获取目录下所有CSV文件\n",
    "    # csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    # csv_files = [f for f in os.listdir(directory) if f.endswith('.csv') and 'adjustlr' in f]\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv') and 'span28_84_84' in f]\n",
    "    \n",
    "    # 存储每个文件的mse均值\n",
    "    mse_means = {}\n",
    "    \n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        \n",
    "        try:\n",
    "            # 读取CSV文件\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # 检查是否存在mse列\n",
    "            if 'mse' in df.columns:\n",
    "                # 计算mse列的均值\n",
    "                mse_mean = df['mse'].mean()\n",
    "                mse_means[file] = mse_mean\n",
    "            else:\n",
    "                print(f\"文件 {file} 中没有 mse 列\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {file} 时出错: {e}\")\n",
    "    \n",
    "    return mse_means\n",
    "\n",
    "# 指定目录\n",
    "directory = 'workresult/10dataset_roll_ewma'\n",
    "\n",
    "# 计算mse均值\n",
    "mse_means = calculate_mse_mean(directory)\n",
    "\n",
    "# 打印结果\n",
    "for file, mean in mse_means.items():\n",
    "    print(f\"文件 {file} 的 mse 均值: {mean}\")\n",
    "\n",
    "# 计算所有文件的mse均值的均值\n",
    "if mse_means:\n",
    "    overall_mse_mean = sum(mse_means.values()) / len(mse_means)\n",
    "    print(f\"所有文件的 mse 均值的均值: {overall_mse_mean}\")\n",
    "else:\n",
    "    print(\"没有找到符合条件的文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"  Data Path:          996.ICU_all_ewma_span20.csvFeatures:           M    \"\n",
    "dataset_pattern = re.compile(r'Data Path:\\s*(.+?)_all_ewma_span\\d+\\.csv')\n",
    "\n",
    "match = dataset_pattern.search(text)\n",
    "if match:\n",
    "    print(\"匹配到的内容:\", match.group(1))\n",
    "else:\n",
    "    print(\"未匹配到任何内容\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# 您的文件路径\n",
    "your_file_path = 'workresult/Top500_all_ewma_Result.csv'\n",
    "# 日志文件路径\n",
    "log_file_path = 'logs/Top500_all_ewma_patchTST_48_48.log'\n",
    "\n",
    "# 正则表达式模式，用于匹配数据集名称\n",
    "dataset_pattern = re.compile(r'Data Path:\\s*(.+?)_all_ewma_span\\d+\\.csv')\n",
    "\n",
    "# 从您的文件中读取 dataset 列\n",
    "def read_datasets_from_file(file_path):\n",
    "    datasets = set()\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            dataset = row.get('dataset')\n",
    "            if dataset:\n",
    "                datasets.add(dataset)\n",
    "    return datasets\n",
    "\n",
    "# 从日志文件中提取 dataset 名称\n",
    "def extract_datasets_from_log(file_path):\n",
    "    datasets = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as log_file:\n",
    "        for line in log_file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            dataset_match = dataset_pattern.search(line)\n",
    "            if dataset_match:\n",
    "                dataset = dataset_match.group(1)\n",
    "                datasets.add(dataset)\n",
    "    return datasets\n",
    "\n",
    "# 读取您的文件中的 dataset 名称\n",
    "your_datasets = read_datasets_from_file(your_file_path)\n",
    "\n",
    "# 从日志文件中提取 dataset 名称\n",
    "log_datasets = extract_datasets_from_log(log_file_path)\n",
    "\n",
    "# 找出您的文件中缺少的 dataset 名称\n",
    "missing_datasets = log_datasets - your_datasets\n",
    "\n",
    "# 打印缺少的 dataset 名称\n",
    "if missing_datasets:\n",
    "    print(\"以下 dataset 在您的文件中缺失：\")\n",
    "    for dataset in missing_datasets:\n",
    "        print(dataset)\n",
    "else:\n",
    "    print(\"您的文件中包含了所有的 dataset。\")\n",
    "\n",
    "# 可选：将缺少的 dataset 名称保存到一个新的文件中\n",
    "# output_file_path = 'missing_datasets.txt'\n",
    "# with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "#     for dataset in missing_datasets:\n",
    "#         output_file.write(f\"{dataset}\\n\")\n",
    "\n",
    "# print(f\"缺少的 dataset 名称已保存到 {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
